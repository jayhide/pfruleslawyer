This is a repo that serves two purposes:
1) To scrape pathfinder 1e rules and content from the https://www.d20pfsrd.com/ website
2) To use RAG over the scraped rules to provide relevant context to an llm which answers questions on pathfinder 1e rules

poetry is used to track dependencies and maintain the environment for running code

## Project Structure

- `rules/` - Scraped markdown files from d20pfsrd.com (legacy)
- `scraper/` - URL discovery, HTML scraping, and markdown conversion scripts
- `scraper/html_cache.db` - SQLite database with cached HTML and markdown
- `manifests/` - Section manifests (JSON) generated by preprocessing
- `db.py` - Unified database access for html_cache.db
- `preprocess_config.json` - Configuration for database-driven preprocessing
- `preprocess_from_db.py` - Process markdown from database using config
- `preprocess_sections.py` - Generates section manifests from markdown files
- `chunking_prompt.py` - Prompt template for LLM-powered section extraction
- `section_extractor.py` - Extracts section content using manifests
- `vector_store.py` - ChromaDB vector store for semantic search
- `vectordb/` - Persisted vector database
- `rules_lawyer.py` - RAG-powered rules Q&A interface

## Section Preprocessing

The markdown files have inconsistent heading structures, so we use an LLM to identify logical sections for RAG retrieval.

### From Files

```bash
# Process all rules files
poetry run python preprocess_sections.py -v

# Process a single file
poetry run python preprocess_sections.py --file combat.md -v
```

### From Database

Process markdown directly from the scraper database using `preprocess_config.json`:

```bash
# Show what would be processed
poetry run python preprocess_from_db.py --stats

# Preview without API calls
poetry run python preprocess_from_db.py --dry-run -v

# Process all configured URLs
poetry run python preprocess_from_db.py -v

# Process specific category only
poetry run python preprocess_from_db.py --category "Combat Feats" -v

# Force reprocess existing manifests
poetry run python preprocess_from_db.py --force
```

### Configuration Format

Edit `preprocess_config.json` to specify URLs to process:

```json
{
  "entries": [
    {
      "url": "https://www.d20pfsrd.com/gamemastering/combat/",
      "mode": "full",
      "category": "Core Rules",
      "name": "Combat Rules"
    },
    {
      "pattern": "https://www.d20pfsrd.com/feats/combat-feats/*",
      "mode": "simple",
      "category": "Combat Feats",
      "name": "Feat: Combat",
      "exclude": ["https://www.d20pfsrd.com/feats/combat-feats/some-bad-feat/"]
    }
  ]
}
```

- `url` - Exact URL match
- `urls` - List of exact URL matches
- `pattern` - Glob pattern (supports `*` wildcard)
- `mode` - `"full"` (multiple sections) or `"simple"` (single section)
- `category` - Human-readable name for filtering
- `name` - (optional) Source name for manifests (overrides auto-generated name)
- `name_prefix` - (optional) Prefix for source name, combined with page title (e.g., `"Skill"` â†’ "Skill: Acrobatics")
- `exclude` - (optional) URLs to skip within a pattern

### Output Format

Each manifest in `manifests/` contains sections with:
- `id` - Unique identifier
- `title` - Human-readable name
- `anchor_heading` - Exact markdown heading to locate the section
- `includes_subheadings` - Subheadings contained in this section
- `description` - Brief summary of the rules covered
- `keywords` - Terms for retrieval matching

## Section Extraction

Use `SectionExtractor` to load and search sections:

```python
from section_extractor import SectionExtractor

extractor = SectionExtractor()
sections = extractor.load_all_sections()  # 301 sections

# Search by text (matches title, description, keywords)
results = extractor.search_by_text("grapple")

# Get specific section by ID
section = extractor.get_section_by_id("flat_footed_condition")
print(section.content)  # Full markdown content
```

Each `Section` object has: `id`, `title`, `description`, `keywords`, `content`, `source_file`, `anchor_heading`

## Vector Search (RAG)

Build and query the semantic search index:

```bash
# Build/rebuild the index
poetry run python vector_store.py --build

# Query for relevant sections
poetry run python vector_store.py -q "how does grappling work"
poetry run python vector_store.py -q "attack of opportunity" -n 10
```

Or use programmatically:

```python
from vector_store import RulesVectorStore

store = RulesVectorStore()
results = store.query("what happens when I fall unconscious", n_results=5)

for r in results:
    print(f"{r['title']} (score: {r['score']:.3f})")
    print(r['content'])
```

## Asking Rules Questions

The main interface for asking Pathfinder rules questions:

```bash
# Single question
poetry run python rules_lawyer.py "How does grappling work?"

# With verbose output (shows retrieved sections)
poetry run python rules_lawyer.py "What is flat-footed?" -v

# Interactive mode
poetry run python rules_lawyer.py
```

The system retrieves the top 5 most relevant rules sections and uses Claude to answer based on those rules.

## Scraper

The `scraper/` directory contains scripts for discovering URLs, scraping HTML, and converting to markdown. See `scraper/CLAUDE.md` for detailed usage.

Key scripts:
- `scraper/fetch_sitemap_urls.py` - Discover URLs from sitemap
- `scraper/filter_urls.py` - Filter out third-party content
- `scraper/scrape_html.py` - Scrape and cache HTML
- `scraper/convert_to_markdown.py` - Convert HTML to markdown
- `scraper/analyze_html.py` - Analyze cached HTML
- `scraper/analyze_urls.py` - Analyze URL path hierarchy with counts

## Database Access

The `db.py` module provides unified read-only access to `scraper/html_cache.db`:

```python
from db import HtmlCacheDB

db = HtmlCacheDB()

# Get markdown for a URL
markdown = db.get_markdown("https://www.d20pfsrd.com/classes/core-classes/fighter/")

# Get all URLs with markdown content
urls = db.get_all_urls_with_markdown()

# Get raw HTML
html = db.get_html("https://www.d20pfsrd.com/classes/core-classes/fighter/")

# Check if URL exists
exists = db.has_url("https://www.d20pfsrd.com/some-url/")

# Get statistics
stats = db.stats()  # {total, with_html, with_markdown, errors}
```

The `SectionExtractor` automatically uses this to fetch markdown when a manifest's `source_path` is a URL.

## Environment

Requires `ANTHROPIC_API_KEY` in `.env` file.
