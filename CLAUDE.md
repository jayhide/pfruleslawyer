This is a repo that serves two purposes:
1) To scrape pathfinder 1e rules and content from the https://www.d20pfsrd.com/ website
2) To use RAG over the scraped rules to provide relevant context to an llm which answers questions on pathfinder 1e rules

poetry is used to track dependencies and maintain the environment for running code

## Project Structure

- `rules/` - Scraped markdown files from d20pfsrd.com
- `manifests/` - Section manifests (JSON) generated by preprocessing
- `url_to_markdown.py` - Scrapes a URL and converts to markdown
- `chunking_prompt.py` - Prompt template for LLM-powered section extraction
- `preprocess_sections.py` - Generates section manifests from markdown files
- `section_extractor.py` - Extracts section content using manifests
- `vector_store.py` - ChromaDB vector store for semantic search
- `vectordb/` - Persisted vector database
- `rules_lawyer.py` - RAG-powered rules Q&A interface

## Section Preprocessing

The markdown files have inconsistent heading structures, so we use an LLM to identify logical sections for RAG retrieval.

### Usage

```bash
# Process all rules files
poetry run python preprocess_sections.py -v

# Process a single file
poetry run python preprocess_sections.py --file combat.md -v
```

### Output Format

Each manifest in `manifests/` contains sections with:
- `id` - Unique identifier
- `title` - Human-readable name
- `anchor_heading` - Exact markdown heading to locate the section
- `includes_subheadings` - Subheadings contained in this section
- `description` - Brief summary of the rules covered
- `keywords` - Terms for retrieval matching

## Section Extraction

Use `SectionExtractor` to load and search sections:

```python
from section_extractor import SectionExtractor

extractor = SectionExtractor()
sections = extractor.load_all_sections()  # 301 sections

# Search by text (matches title, description, keywords)
results = extractor.search_by_text("grapple")

# Get specific section by ID
section = extractor.get_section_by_id("flat_footed_condition")
print(section.content)  # Full markdown content
```

Each `Section` object has: `id`, `title`, `description`, `keywords`, `content`, `source_file`, `anchor_heading`

## Vector Search (RAG)

Build and query the semantic search index:

```bash
# Build/rebuild the index
poetry run python vector_store.py --build

# Query for relevant sections
poetry run python vector_store.py -q "how does grappling work"
poetry run python vector_store.py -q "attack of opportunity" -n 10
```

Or use programmatically:

```python
from vector_store import RulesVectorStore

store = RulesVectorStore()
results = store.query("what happens when I fall unconscious", n_results=5)

for r in results:
    print(f"{r['title']} (score: {r['score']:.3f})")
    print(r['content'])
```

## Asking Rules Questions

The main interface for asking Pathfinder rules questions:

```bash
# Single question
poetry run python rules_lawyer.py "How does grappling work?"

# With verbose output (shows retrieved sections)
poetry run python rules_lawyer.py "What is flat-footed?" -v

# Interactive mode
poetry run python rules_lawyer.py
```

The system retrieves the top 5 most relevant rules sections and uses Claude to answer based on those rules.

## URL Discovery and Filtering

Scripts for discovering all URLs from d20pfsrd.com and filtering out third-party content.

### Fetching URLs from Sitemap

```bash
# Fetch all page URLs from sitemap
poetry run python fetch_sitemap_urls.py --pages-only

# Include taxonomy sitemaps
poetry run python fetch_sitemap_urls.py

# Also crawl category pages for additional discovery
poetry run python fetch_sitemap_urls.py --crawl
```

Output: `sitemap_urls.txt` with all discovered URLs.

Note: The d20pfsrd server returns HTTP 404 status codes but still serves valid sitemap XML content. The script handles this by parsing response content regardless of status code.

### Filtering URLs

```bash
# Show what would be filtered (dry run)
poetry run python filter_urls.py --stats

# Filter and save
poetry run python filter_urls.py -i sitemap_urls.txt -o filtered_urls.txt
```

Current exclusion patterns:
- `/3rd-party`, `3pp-` - Third-party publisher content
- `/extras/` - Community creations, review queues
- `/work-area/` - Internal work pages
- `/alternative-rule-systems/` - Mixed first/third-party, excluded for now
- `/subscribe/`, `/gaming-accessories/`, `/d20pfsrd-com-publishing-products/` - Non-rules content
- Third-party archetypes detected by checking for `paizo` in archetype folder paths

## HTML Scraping

```bash
# Scrape all filtered URLs (stores in html_cache.db)
poetry run python scrape_html.py

# Use more workers for faster scraping
poetry run python scrape_html.py --workers 5

# Check progress
poetry run python scrape_html.py --stats

# Retrieve cached HTML for a specific URL
poetry run python scrape_html.py --get "https://www.d20pfsrd.com/feats/combat-feats/power-attack-combat/"
```

## Markdown Conversion

Convert cached HTML to clean markdown:

```bash
# Convert all cached HTML to markdown
poetry run python convert_to_markdown.py

# Get markdown for a specific URL
poetry run python convert_to_markdown.py --get "https://www.d20pfsrd.com/feats/combat-feats/power-attack-combat/"

# Preview conversion without saving
poetry run python convert_to_markdown.py --preview URL

# Check conversion stats
poetry run python convert_to_markdown.py --stats

# Force re-convert all pages
poetry run python convert_to_markdown.py --force
```

Content extraction:
- Starts at first `<h1>` tag
- Stops at copyright section (`div.section15`) or sidebar fallback (`div.right-sidebar`, etc.)
- Preserves links (useful for LLM context)
- Use `strip_links(markdown)` when generating embeddings to avoid confusing semantic search

Programmatic usage:
```python
from convert_to_markdown import MarkdownCache, strip_links

cache = MarkdownCache()
markdown = cache.get_markdown("https://www.d20pfsrd.com/...")

# For embeddings, strip links
text_for_embedding = strip_links(markdown)
```

## HTML Analysis

Analyze cached HTML to test content extraction heuristics:

```bash
# Analyze all cached pages
poetry run python analyze_html.py

# Show pages missing h1 tags or copyright sections
poetry run python analyze_html.py --show-missing-h1
poetry run python analyze_html.py --show-missing-copyright

# Show pages with very short content
poetry run python analyze_html.py --show-short 200

# Show pages with multiple h1 tags
poetry run python analyze_html.py --show-multi-h1
```

Reports statistics on:
- H1 tag presence (starting point for content extraction)
- Copyright section presence (ending point for content extraction)
- Content length distribution when converted to markdown

## Environment

Requires `ANTHROPIC_API_KEY` in `.env` file.
